{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Filling a tensor with 'n' number of random nos.**"
      ],
      "metadata": {
        "id": "yFIqWsOIUzp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bckFDZKbpkEl",
        "outputId": "761bb05a-20c7-4e3d-eb76-b000770dfef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4058, 1.9036, 0.7993, 1.7144, 0.3726], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch as t\n",
        "\n",
        "x = t.randn(5, requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Addition Operation On A Tensor**\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "jqpLUD0MU-3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = x+2\n",
        "print(y)"
      ],
      "metadata": {
        "id": "Jygyx5trrigE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8a319b-1bda-4ec9-a868-f214cf47b6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.4058, 3.9036, 2.7993, 3.7144, 2.3726], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Multiplication Operation On A Tensor**"
      ],
      "metadata": {
        "id": "1-c9CV5ZVG5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = y*y*2 # here 'z' is having a vector output as it is dependent on y:\n",
        "          # due to this we can't apply the backward function or grad operation\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_g40ARY9Kwx",
        "outputId": "120469b8-63b8-4a37-84ef-dd79acb41fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([11.5758, 30.4756, 15.6722, 27.5934, 11.2587], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Mean Operation On A Tensor**"
      ],
      "metadata": {
        "id": "BnT_eNNoVPVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = z.mean() # Applying the mean operation\n",
        "print(z) # - here 'z' has a scalar output, and we can safely apply the backward function or grad operation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnwuonYd9QwQ",
        "outputId": "137b31da-cf58-4edc-af18-7286f4e53d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(19.3151, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. Backward Propagation For Gradient Descent Computation**"
      ],
      "metadata": {
        "id": "jWL0CejIVU0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward() #Backward propagation function - will calculate dz/dx - x will have gradient stored in it\n",
        "print(x.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMcEUEvr9Vb0",
        "outputId": "2871a527-b31c-42e4-d4c2-f28be924715a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.9246, 3.1228, 2.2394, 2.9715, 1.8981])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **6. Demonstrating That For Backward Propagation, Input Paramater Should Be Scalar**"
      ],
      "metadata": {
        "id": "M6KDAf7NqBP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = y*y*2\n"
      ],
      "metadata": {
        "id": "cNSV96Vf-tEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "SsilH2YNAZyb",
        "outputId": "ebe6be0e-f9c4-4081-ba41-91c1a6efe582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fdaeb047dc58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7. A Custom Workaround For Backward Propagation - Passing Scalar Parameter**"
      ],
      "metadata": {
        "id": "8bZ4CBNkqRui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rectifying the above error\n",
        "\n",
        "v = t.tensor([0.1,1.0,0.001,0.01,0.0001], dtype=t.float32)\n",
        "w.backward(v) # since w is a vector we pass 'v' which is a scalar for applying backward propagation\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JYY2IxUAkvQ",
        "outputId": "0ec22795-0aa0-4d39-969a-8c47b3ba945a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.8870, 18.7371,  2.2506,  3.1201,  1.8990])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **8. 3 Different Ways of Working Without Gradients**"
      ],
      "metadata": {
        "id": "mfSzOhsjq20k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working without gradient functions [Option #1]:\n",
        "\n",
        "x.requires_grad_(False) #will remove the gradient requirement from 'x' var. in place\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51sR9RwPBAE1",
        "outputId": "05b2f451-fc61-4fea-d67b-5f4e904cebd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4058, 1.9036, 0.7993, 1.7144, 0.3726])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(True)\n",
        "\n",
        "# Option #2:\n",
        "\n",
        "qw = x.detach()\n",
        "print(qw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d88NsRaCFSj",
        "outputId": "0b0327ca-79c7-4630-e2c6-5efed756be90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4058, 1.9036, 0.7993, 1.7144, 0.3726])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option #3: - this one can enable working in modular functions\n",
        "\n",
        "with t.no_grad():\n",
        "  rw = x+2\n",
        "  print(rw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OMRXgFJCbGK",
        "outputId": "cfd7c355-e70a-4bae-afc8-78e4d76fa0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.4058, 3.9036, 2.7993, 3.7144, 2.3726])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **9. Emptying Gradients**"
      ],
      "metadata": {
        "id": "ndXP4yioq_B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = t.rand(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(2):\n",
        "  model_output = (weights*3).sum()\n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "  weights.grad.zero_() #In training model in each iteration, we need to make sure to apply this step:\n",
        "                       # of clearing the gradients [or filling it with zeros] so that it avoids the creation of imprecise gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMzYN42VCyuO",
        "outputId": "c467fe52-b6e2-4b4c-81a1-fbf047c68701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    }
  ]
}