{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling Linear Regression Manually"
      ],
      "metadata": {
        "id": "YoNzSIpCopVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performing four-pronged steps: (1) Forward Pass + Prediction (2) Loss Computation After Prediction (3) Gradient Computation (4) Backward Pass To Correct Weights + Biases Using Gradients and Learning Rate"
      ],
      "metadata": {
        "id": "UutpB-Seo2oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss computation\n",
        "\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()"
      ],
      "metadata": {
        "id": "PvpjZlwLkWcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7bVLNqsRtBV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w=0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Prediction\n",
        "\n",
        "def forward_pass(x):\n",
        "  return w*x"
      ],
      "metadata": {
        "id": "Xvl-jav1kMCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient of Loss Calculation\n",
        "\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x,y_predicted-y).mean()"
      ],
      "metadata": {
        "id": "SVHlT4PFkt3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Model Now\n",
        "\n",
        "n_iters = 15 # no. of iterations\n",
        "learning_rate = 0.01 # for controlled gradient descent\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "  #making the prediction here --> forward passing\n",
        "\n",
        "  y_pred = forward_pass(X)\n",
        "\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #computing the gradient here\n",
        "\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  # Updating the weights\n",
        "\n",
        "  w -= learning_rate*dw\n",
        "\n",
        "  if epoch % 1 ==0:\n",
        "    print(f'Epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after the training: f(5) = {forward_pass(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeWQjLtqlEpq",
        "outputId": "f54fd237-835f-48d9-dfe0-ace119eb1af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w = 1.200, loss = 30.00000000\n",
            "Epoch 2: w = 1.680, loss = 4.79999924\n",
            "Epoch 3: w = 1.872, loss = 0.76800019\n",
            "Epoch 4: w = 1.949, loss = 0.12288000\n",
            "Epoch 5: w = 1.980, loss = 0.01966083\n",
            "Epoch 6: w = 1.992, loss = 0.00314574\n",
            "Epoch 7: w = 1.997, loss = 0.00050331\n",
            "Epoch 8: w = 1.999, loss = 0.00008053\n",
            "Epoch 9: w = 1.999, loss = 0.00001288\n",
            "Epoch 10: w = 2.000, loss = 0.00000206\n",
            "Epoch 11: w = 2.000, loss = 0.00000033\n",
            "Epoch 12: w = 2.000, loss = 0.00000005\n",
            "Epoch 13: w = 2.000, loss = 0.00000001\n",
            "Epoch 14: w = 2.000, loss = 0.00000000\n",
            "Epoch 15: w = 2.000, loss = 0.00000000\n",
            "Prediction after the training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling Linear Regression Using PyTorch Inbuilt Modules"
      ],
      "metadata": {
        "id": "g5UQsriWpidB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performing four-pronged steps:\n",
        "\n",
        "##### (1) Forward Pass + Prediction ***[Done Manually] ***\n",
        "\n",
        "##### (2) Loss Computation After Prediction ***[Done Manually]***\n",
        "\n",
        "##### (3) Gradient Computation ***[Used PyTorch Autograd Here] ***\n",
        "\n",
        "##### (4) Backward Pass To Correct Weights + Biases Using Gradients and Learning Rate **[Done Manually Again]**"
      ],
      "metadata": {
        "id": "8Ct_Nixo3A9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "rHgG4sH1poel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0, dtype=torch.float32, requires_grad=True)\n"
      ],
      "metadata": {
        "id": "FMbwbQopxT5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss_function(y, y_pred):\n",
        "  return ((y-y_pred)**2).mean() # MSE\n"
      ],
      "metadata": {
        "id": "shcpkX6Nx2G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  loss_val = loss_function(Y,y_pred)\n",
        "\n",
        "  loss_val.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate*w.grad #Updating weights after detaching from computational graph\n",
        "\n",
        "  w.grad.zero_() # Clearing gradients after each epoch (important step)\n",
        "\n",
        "  if epoch%10 == 0:\n",
        "    print(f'Epoch #{epoch+1}: w = {w:.2f}, loss = {loss_val:.8f}')\n",
        "\n",
        "print(f'Predicted Value After Training ==> f(25) = {forward(25):.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl4UInk9ymHq",
        "outputId": "a0be7880-3f02-4a7b-c96f-3d98a261d984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1: w = 0.55, loss = 21.67499924\n",
            "Epoch #11: w = 1.72, loss = 0.84011245\n",
            "Epoch #21: w = 1.94, loss = 0.03256231\n",
            "Epoch #31: w = 1.99, loss = 0.00126211\n",
            "Epoch #41: w = 2.00, loss = 0.00004891\n",
            "Epoch #51: w = 2.00, loss = 0.00000190\n",
            "Epoch #61: w = 2.00, loss = 0.00000007\n",
            "Epoch #71: w = 2.00, loss = 0.00000000\n",
            "Epoch #81: w = 2.00, loss = 0.00000000\n",
            "Epoch #91: w = 2.00, loss = 0.00000000\n",
            "Predicted Value After Training ==> f(25) = 50.00\n"
          ]
        }
      ]
    }
  ]
}